{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "FuXIFTFapAMI",
    "outputId": "c8ced1ad-c7b3-44ba-807b-26d7d13906bc"
   },
   "outputs": [],
   "source": [
    "# You only need to run this once per machine\n",
    "!pip install -q -U bitsandbytes\n",
    "!pip install -q -U git+https://github.com/huggingface/transformers.git\n",
    "!pip install -q -U git+https://github.com/huggingface/peft.git\n",
    "!pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
    "!pip install -q -U datasets scipy ipywidgets matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42eef161636d497caec43d7b8f57c323",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d710815d34fd4f52995ba8700cf9c6b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "train_dataset = load_dataset('json', data_files='train_1000.jsonl', split='train')\n",
    "eval_dataset = load_dataset('json', data_files='test_1000.jsonl', split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "DDqUNyIoyRgo"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "  ········\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33misaiahhoussou2\u001b[0m (\u001b[33misaiahhoussou2-camp-hill-high-school\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "!pip install -q wandb -U\n",
    "\n",
    "import wandb, os\n",
    "wandb.login()\n",
    "\n",
    "wandb_project = \"journal-finetune\"\n",
    "if len(wandb_project) > 0:\n",
    "    os.environ[\"WANDB_PROJECT\"] = wandb_project"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "uhw8JiOr3m18"
   },
   "source": [
    "### Formatting prompts\n",
    "Then create a `formatting_func` to structure training examples as prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "f-fJR0MlQiTD"
   },
   "outputs": [],
   "source": [
    "def formatting_func(example):\n",
    "    text = f\"### Question: {example['input']}\\n ### Answer: {example['output']}\"\n",
    "    return text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "shz8Xdv-yRgf"
   },
   "source": [
    "### 2. Load Base Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "MJ-5idQwzvg-"
   },
   "source": [
    "Let's now load Mistral - mistralai/Mistral-7B-v0.1 - using 4-bit quantization!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23ad9f69252f4fb19155c1b1a7ecfac0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "45524c98039a46d5b7745ad7cb638d2f"
     ]
    },
    "id": "E0Nl5mWL0k2T",
    "outputId": "47b6b01d-e9f2-4b70-919c-17ae64993843"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cbbf4ace3584994b46e76194ff0dada",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "base_model_id = \"mistralai/Mistral-7B-v0.1\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "token = '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'\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_id,\n",
    "    quantization_config=bnb_config, \n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    # attn_implementation=\"flash_attention_2\",\n",
    "    token=token\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "haSUDD9HyRgf",
    "outputId": "22ee95db-2974-4ab0-e0c7-444d04d3e838"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    base_model_id,\n",
    "    padding_side=\"left\",\n",
    "    add_eos_token=True,\n",
    "    add_bos_token=True,\n",
    "    token=token\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def generate_and_tokenize_prompt(prompt):\n",
    "    return tokenizer(formatting_func(prompt))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "WHnKLcq4yRgg"
   },
   "source": [
    "Reformat the prompt and tokenize each sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "S3iLAwLh3m19"
   },
   "outputs": [],
   "source": [
    "tokenized_train_dataset = train_dataset.map(generate_and_tokenize_prompt)\n",
    "tokenized_val_dataset = eval_dataset.map(generate_and_tokenize_prompt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "O6ewk27p3m19"
   },
   "source": [
    "Let's get a distribution of our dataset lengths, so we can determine the appropriate `max_length` for our input tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "BA8M9yfC3m19",
    "outputId": "99c6d302-9bb6-47b1-cae9-a1cd870b4770"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1IAAAIjCAYAAAAJLyrXAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAASM1JREFUeJzt3XlcVdXex/HvAWQQBBwZFAGVnC2H8pJc00RxiPTqvaZZqWk2aM5lVpqaZlqZ2aCNmmVallp2r5Zj3spMzaHBcJ4BezJAHABhPX/04tyOoLLxyDng5/16ndfTWXudvX97seHxe9fe69iMMUYAAAAAgCLzcHUBAAAAAFDaEKQAAAAAwCKCFAAAAABYRJACAAAAAIsIUgAAAABgEUEKAAAAACwiSAEAAACARQQpAAAAALCIIAUAAAAAFhGkAFzzJkyYIJvNViLHatOmjdq0aWN/v379etlsNn388cclcvx+/fopKiqqRI5VXJmZmRo4cKBCQ0Nls9k0fPhwV5fkdCX9c7+clStX6oYbbpCvr69sNpvS0tIK7Tdv3jzZbDYdPHiwROu7GqycS1RUlPr163fVawJQuhCkAJQp+f84yn/5+voqPDxcCQkJmjVrlk6dOuWU4xw/flwTJkzQ9u3bnbI/Z3Ln2orimWee0bx58/Tggw/qvffe0913333RvlFRUbrttttKsDprPvjgA82cOdPVZVzS77//rp49e8rPz0+vvvqq3nvvPfn7+7u6rCL55ZdfNGHChDIR7ACUPl6uLgAAroZJkyYpOjpaOTk5SklJ0fr16zV8+HDNmDFDn332mZo0aWLv++STT+qxxx6ztP/jx49r4sSJioqK0g033FDkz3355ZeWjlMcl6rtzTffVF5e3lWv4UqsXbtWf/vb3/TUU0+5upQr9sEHH+inn35y61m1zZs369SpU3r66acVHx9/yb533323evXqJR8fnxKq7tJ++eUXTZw4UW3atLE80+pu5wKg9CFIASiTOnXqpBYtWtjfjx07VmvXrtVtt92m22+/Xbt27ZKfn58kycvLS15eV/fP4ZkzZ1S+fHl5e3tf1eNcTrly5Vx6/KI4ceKEGjRo4OoyrhknTpyQJAUHB1+2r6enpzw9Pa9yRSWjLJ0LANfg1j4A14xbb71V48aN06FDh/T+++/b2wt7RmrVqlWKi4tTcHCwAgICVLduXT3++OOS/ny+5cYbb5Qk9e/f334b4bx58yT9+RxUo0aNtHXrVrVu3Vrly5e3f/bCZ6Ty5ebm6vHHH1doaKj8/f11++2368iRIw59Lvacxl/3ebnaCntG6vTp0xo1apQiIiLk4+OjunXr6vnnn5cxxqGfzWbTkCFDtGzZMjVq1Eg+Pj5q2LChVq5cWfiAX+DEiRMaMGCAQkJC5Ovrq+uvv17vvvuufXv+c0MHDhzQv//9b3vtzrht6/3331fz5s3l5+enSpUqqVevXgXGN//n9ssvv6ht27YqX768qlevrunTpxfY36FDh3T77bfL399f1apV04gRI/TFF1/IZrNp/fr19v39+9//1qFDh+zncuHY5+XlacqUKapRo4Z8fX3Vrl077d2716HPnj171KNHD4WGhsrX11c1atRQr169lJ6eftnzXrx4sf28q1SporvuukvHjh1zOOe+fftKkm688UbZbLZLPgtU2HNF+bdXfv3117rpppvk6+urWrVqaf78+YV+dsOGDbr//vtVuXJlBQYG6p577tEff/zh0Ndms2nChAkFjv/X34F58+bpX//6lySpbdu29jHOH//LKexcjDGaPHmyatSoofLly6tt27b6+eefC3w2JydHEydOVExMjHx9fVW5cmXFxcVp1apVRTo2gLKBGSkA15S7775bjz/+uL788kvdd999hfb5+eefddttt6lJkyaaNGmSfHx8tHfvXn3zzTeSpPr162vSpEkaP368Bg0apL///e+SpJtvvtm+j99//12dOnVSr169dNdddykkJOSSdU2ZMkU2m01jxozRiRMnNHPmTMXHx2v79u32mbOiKEptf2WM0e23365169ZpwIABuuGGG/TFF1/okUce0bFjx/Tiiy869P/666+1ZMkSPfTQQ6pQoYJmzZqlHj166PDhw6pcufJF6zp79qzatGmjvXv3asiQIYqOjtbixYvVr18/paWladiwYapfv77ee+89jRgxQjVq1NCoUaMkSVWrVi3y+RdmypQpGjdunHr27KmBAwfqt99+08svv6zWrVtr27ZtDjMxf/zxhzp27Kju3burZ8+e+vjjjzVmzBg1btxYnTp1kvRn8Lz11luVnJysYcOGKTQ0VB988IHWrVvncNwnnnhC6enpOnr0qH0cAwICHPo8++yz8vDw0OjRo5Wenq7p06erT58+2rRpkyQpOztbCQkJysrK0sMPP6zQ0FAdO3ZMn3/+udLS0hQUFHTR8543b5769++vG2+8UVOnTlVqaqpeeuklffPNN/bzfuKJJ1S3bl298cYb9ttha9eubXmM9+7dq3/+858aMGCA+vbtq3feeUf9+vVT8+bN1bBhQ4e+Q4YMUXBwsCZMmKCkpCTNnj1bhw4dsgfpomrdurWGDh2qWbNm6fHHH1f9+vUlyf5/i2P8+PGaPHmyOnfurM6dO+uHH35Qhw4dlJ2d7dBvwoQJmjp1qgYOHKibbrpJGRkZ2rJli3744Qe1b9++2McHUMoYAChD5s6daySZzZs3X7RPUFCQadq0qf39U089Zf765/DFF180ksxvv/120X1s3rzZSDJz584tsO2WW24xksycOXMK3XbLLbfY369bt85IMtWrVzcZGRn29o8++shIMi+99JK9LTIy0vTt2/ey+7xUbX379jWRkZH298uWLTOSzOTJkx36/fOf/zQ2m83s3bvX3ibJeHt7O7Tt2LHDSDIvv/xygWP91cyZM40k8/7779vbsrOzTWxsrAkICHA498jISNOlS5dL7q+ofQ8ePGg8PT3NlClTHNp//PFH4+Xl5dCe/3ObP3++vS0rK8uEhoaaHj162NteeOEFI8ksW7bM3nb27FlTr149I8msW7fO3t6lSxeH8c6X/3OvX7++ycrKsre/9NJLRpL58ccfjTHGbNu2zUgyixcvvvxg/EV2drapVq2aadSokTl79qy9/fPPPzeSzPjx4+1tRfmdubDvgQMH7G2RkZFGktmwYYO97cSJE8bHx8eMGjWqwGebN29usrOz7e3Tp083ksynn35qb5NknnrqqQLHv/B3YPHixQXGvKguPJcTJ04Yb29v06VLF5OXl2fv9/jjjxtJDse9/vrri3yNAii7uLUPwDUnICDgkqv35c9QfPrpp8VemMHHx0f9+/cvcv977rlHFSpUsL//5z//qbCwMP3nP/8p1vGL6j//+Y88PT01dOhQh/ZRo0bJGKMVK1Y4tMfHxzvMWDRp0kSBgYHav3//ZY8TGhqq3r1729vKlSunoUOHKjMzU1999ZUTzqagJUuWKC8vTz179tT//d//2V+hoaGKiYkpMIsUEBCgu+66y/7e29tbN910k8P5rVy5UtWrV9ftt99ub/P19b3oDOel9O/f3+G5ufwZxPzj5c84ffHFFzpz5kyR97tlyxadOHFCDz30kHx9fe3tXbp0Ub169fTvf//bcq2X0qBBA3vt0p+ziHXr1i30uhg0aJDDs3oPPvigvLy8rvq1fjmrV69Wdna2Hn74YYeZscIWCgkODtbPP/+sPXv2lGCFANwNQQrANSczM9MhtFzojjvuUKtWrTRw4ECFhISoV69e+uijjyyFqurVq1taWCImJsbhvc1mU506da76ss6HDh1SeHh4gfHIvz3q0KFDDu01a9YssI+KFSsWeMalsOPExMTIw8Px/+1c7DjOsmfPHhljFBMTo6pVqzq8du3aZV9oIV+NGjUK3F524fkdOnRItWvXLtCvTp06luu7cDwrVqwoSfbjRUdHa+TIkXrrrbdUpUoVJSQk6NVXX73s81H541m3bt0C2+rVq+f08bZyXVx4rQcEBCgsLMzlS5jnj8mF9VWtWtX+c8k3adIkpaWl6brrrlPjxo31yCOPaOfOnSVWKwD3QJACcE05evSo0tPTL/mPXj8/P23YsEGrV6/W3XffrZ07d+qOO+5Q+/btlZubW6TjWHmuqagu9vxIUWtyhoutcmYuWJjCXeTl5clms2nlypVatWpVgdfrr7/u0L+kz68ox3vhhRe0c+dOPf744zp79qyGDh2qhg0b6ujRo1elpuIoqXEryWv9Ulq3bq19+/bpnXfeUaNGjfTWW2+pWbNmeuutt1xdGoASRJACcE157733JEkJCQmX7Ofh4aF27dppxowZ+uWXXzRlyhStXbvWfiuYlYfii+LCW4SMMdq7d6/DKm8VK1ZUWlpagc9eOLtgpbbIyEgdP368wK2Ov/76q327M0RGRmrPnj0FZvWcfZwL1a5dW8YYRUdHKz4+vsDrb3/7m+V9RkZGat++fQVCwoWr7UnOu04aN26sJ598Uhs2bNB///tfHTt2THPmzLlkjZKUlJRUYFtSUtJVG++iuPBaz8zMVHJy8mWv9ezsbCUnJzu0OfP3MH9MLqzvt99+K3RmrVKlSurfv78WLlyoI0eOqEmTJoWuNAig7CJIAbhmrF27Vk8//bSio6PVp0+fi/Y7efJkgbb8L7bNysqSJPn7+0tSocGmOObPn+8QZj7++GMlJyfbV4qT/gwF3333ncMKYp9//nmBZbyt1Na5c2fl5ubqlVdecWh/8cUXZbPZHI5/JTp37qyUlBR9+OGH9rbz58/r5ZdfVkBAgG655RanHOdC3bt3l6enpyZOnFgg+Bhj9Pvvv1veZ0JCgo4dO6bPPvvM3nbu3Dm9+eabBfr6+/sXaZnyi8nIyND58+cd2ho3biwPDw/7tViYFi1aqFq1apozZ45DvxUrVmjXrl3q0qVLsWu6Um+88YZycnLs72fPnq3z588XuNY3bNhQ4HMXzkg58/cwPj5e5cqV08svv+xwrcycObNA3wuvm4CAANWpU+eSPxMAZQ/LnwMok1asWKFff/1V58+fV2pqqtauXatVq1YpMjJSn332mcMD+BeaNGmSNmzYoC5duigyMlInTpzQa6+9pho1aiguLk7Sn//QCw4O1pw5c1ShQgX5+/urZcuWio6OLla9lSpVUlxcnPr376/U1FTNnDlTderUcVjAYODAgfr444/VsWNH9ezZU/v27dP7779fYLlqK7UlJiaqbdu2euKJJ3Tw4EFdf/31+vLLL/Xpp59q+PDhxVoKuzCDBg3S66+/rn79+mnr1q2KiorSxx9/rG+++UYzZ8685DNrl7N3715Nnjy5QHvTpk3VpUsXTZ48WWPHjtXBgwfVrVs3VahQQQcOHNDSpUs1aNAgjR492tLx7r//fr3yyivq3bu3hg0bprCwMC1YsMB+Tf11lqR58+b68MMPNXLkSN14440KCAhQYmJikY+1du1aDRkyRP/617903XXX6fz583rvvffk6empHj16XPRz5cqV07Rp09S/f3/dcsst6t27t33586ioKI0YMcLSOTtTdna22rVrp549eyopKUmvvfaa4uLiHBbvGDhwoB544AH16NFD7du3144dO/TFF1+oSpUqDvu64YYb5OnpqWnTpik9PV0+Pj669dZbVa1aNct1Va1aVaNHj9bUqVN12223qXPnztq2bZtWrFhR4LgNGjRQmzZt1Lx5c1WqVElbtmzRxx9/rCFDhhRvUACUTq5ZLBAAro78JY3zX97e3iY0NNS0b9/evPTSSw7LbOe7cPnzNWvWmK5du5rw8HDj7e1twsPDTe/evc3u3bsdPvfpp5+aBg0aGC8vL4flxm+55RbTsGHDQuu72PLnCxcuNGPHjjXVqlUzfn5+pkuXLubQoUMFPv/CCy+Y6tWrGx8fH9OqVSuzZcuWAvu8VG0XLn9ujDGnTp0yI0aMMOHh4aZcuXImJibGPPfccw5LQBvz55LUgwcPLlDTxZZlv1Bqaqrp37+/qVKlivH29jaNGzcudIl2q8uf//Xn/dfXgAED7P0++eQTExcXZ/z9/Y2/v7+pV6+eGTx4sElKSrL3udjPrbAx279/v+nSpYvx8/MzVatWNaNGjTKffPKJkWS+++47e7/MzExz5513muDgYCPJvp/8n/uFy5ofOHDA4ee1f/9+c++995ratWsbX19fU6lSJdO2bVuzevXqIo3Phx9+aJo2bWp8fHxMpUqVTJ8+fczRo0cd+jhj+fPCfl4XXpf5n/3qq6/MoEGDTMWKFU1AQIDp06eP+f333x0+m5uba8aMGWOqVKliypcvbxISEszevXsLvdbefPNNU6tWLePp6WlpKfTCziU3N9dMnDjRhIWFGT8/P9OmTRvz008/FTju5MmTzU033WSCg4ONn5+fqVevnpkyZYrDsu4Ayj6bMW76hDAAAKXIzJkzNWLECB09elTVq1d3dTluJ/8Lgjdv3qwWLVq4uhwAuGI8IwUAgEVnz551eH/u3Dm9/vrriomJIUQBwDWCZ6QAALCoe/fuqlmzpm644Qalp6fr/fff16+//qoFCxa4urRrXmZmpjIzMy/Zp2rVqhddsh0AioogBQCARQkJCXrrrbe0YMEC5ebmqkGDBlq0aJHuuOMOV5d2zXv++ec1ceLES/Y5cOCAw3LrAFAcPCMFAADKjP3792v//v2X7BMXF3fJlTsBoCgIUgAAAABgEYtNAAAAAIBFPCMlKS8vT8ePH1eFChUcvkgRAAAAwLXFGKNTp04pPDxcHh4Xn3ciSEk6fvy4IiIiXF0GAAAAADdx5MgR1ahR46LbCVKSKlSoIOnPwQoMDHRxNQAAAABcJSMjQxEREfaMcDEEKcl+O19gYCBBCgAAAMBlH/lhsQkAAAAAsIggBQAAAAAWEaQAAAAAwCKCFAAAAABYRJACAAAAAIsIUgAAAABgkUuD1IYNG5SYmKjw8HDZbDYtW7bMYbsxRuPHj1dYWJj8/PwUHx+vPXv2OPQ5efKk+vTpo8DAQAUHB2vAgAHKzMwswbMAAAAAcK1xaZA6ffq0rr/+er366quFbp8+fbpmzZqlOXPmaNOmTfL391dCQoLOnTtn79OnTx/9/PPPWrVqlT7//HNt2LBBgwYNKqlTAAAAAHANshljjKuLkP78wqulS5eqW7dukv6cjQoPD9eoUaM0evRoSVJ6erpCQkI0b9489erVS7t27VKDBg20efNmtWjRQpK0cuVKde7cWUePHlV4eHiRjp2RkaGgoCClp6fzhbwAAADANayo2cBtn5E6cOCAUlJSFB8fb28LCgpSy5YttXHjRknSxo0bFRwcbA9RkhQfHy8PDw9t2rTpovvOyspSRkaGwwsAAAAAisptg1RKSookKSQkxKE9JCTEvi0lJUXVqlVz2O7l5aVKlSrZ+xRm6tSpCgoKsr8iIiKcXD0AAACAssxtg9TVNHbsWKWnp9tfR44ccXVJAAAAAEoRtw1SoaGhkqTU1FSH9tTUVPu20NBQnThxwmH7+fPndfLkSXufwvj4+CgwMNDhBQAAAABF5bZBKjo6WqGhoVqzZo29LSMjQ5s2bVJsbKwkKTY2Vmlpadq6dau9z9q1a5WXl6eWLVuWeM0AAAAArg1erjx4Zmam9u7da39/4MABbd++XZUqVVLNmjU1fPhwTZ48WTExMYqOjta4ceMUHh5uX9mvfv366tixo+677z7NmTNHOTk5GjJkiHr16lXkFfsAAAAAwCqXBqktW7aobdu29vcjR46UJPXt21fz5s3To48+qtOnT2vQoEFKS0tTXFycVq5cKV9fX/tnFixYoCFDhqhdu3by8PBQjx49NGvWrBI/FwAAAADXDrf5HilX4nukAAAAAEhl4HukAAAAAMBdEaQAAAAAwCKXPiMFAAAKl5jo6gocLV/u6goAwL0wIwUAAAAAFhGkAAAAAMAighQAAAAAWESQAgAAAACLCFIAAAAAYBFBCgAAAAAsIkgBAAAAgEUEKQAAAACwiCAFAAAAABYRpAAAAADAIoIUAAAAAFhEkAIAAAAAiwhSAAAAAGARQQoAAAAALCJIAQAAAIBFBCkAAAAAsIggBQAAAAAWebm6AAAAACsSE11dwf8sX+7qCgC4CjNSAAAAAGARQQoAAAAALCJIAQAAAIBFBCkAAAAAsIggBQAAAAAWEaQAAAAAwCKCFAAAAABYRJACAAAAAIsIUgAAAABgEUEKAAAAACwiSAEAAACARQQpAAAAALCIIAUAAAAAFhGkAAAAAMAighQAAAAAWESQAgAAAACLCFIAAAAAYBFBCgAAAAAsIkgBAAAAgEUEKQAAAACwiCAFAAAAABZ5uboAAADcRWKiqysAAJQWzEgBAAAAgEUEKQAAAACwiCAFAAAAABbxjBQAALgsnh8DAEfMSAEAAACARQQpAAAAALCIIAUAAAAAFhGkAAAAAMAighQAAAAAWESQAgAAAACLCFIAAAAAYBFBCgAAAAAsIkgBAAAAgEUEKQAAAACwiCAFAAAAABYRpAAAAADAIoIUAAAAAFhEkAIAAAAAiwhSAAAAAGARQQoAAAAALCJIAQAAAIBFBCkAAAAAsIggBQAAAAAWEaQAAAAAwCKCFAAAAABYRJACAAAAAIsIUgAAAABgEUEKAAAAACwiSAEAAACARQQpAAAAALCIIAUAAAAAFhGkAAAAAMAighQAAAAAWESQAgAAAACLCFIAAAAAYBFBCgAAAAAscusglZubq3Hjxik6Olp+fn6qXbu2nn76aRlj7H2MMRo/frzCwsLk5+en+Ph47dmzx4VVAwAAACjr3DpITZs2TbNnz9Yrr7yiXbt2adq0aZo+fbpefvlle5/p06dr1qxZmjNnjjZt2iR/f38lJCTo3LlzLqwcAAAAQFnm5eoCLuXbb79V165d1aVLF0lSVFSUFi5cqO+//17Sn7NRM2fO1JNPPqmuXbtKkubPn6+QkBAtW7ZMvXr1KnS/WVlZysrKsr/PyMi4ymcCAAAAoCxx6xmpm2++WWvWrNHu3bslSTt27NDXX3+tTp06SZIOHDiglJQUxcfH2z8TFBSkli1bauPGjRfd79SpUxUUFGR/RUREXN0TAQAAAFCmuPWM1GOPPaaMjAzVq1dPnp6eys3N1ZQpU9SnTx9JUkpKiiQpJCTE4XMhISH2bYUZO3asRo4caX+fkZFBmAIAAABQZG4dpD766CMtWLBAH3zwgRo2bKjt27dr+PDhCg8PV9++fYu9Xx8fH/n4+DixUgAAAADXErcOUo888ogee+wx+7NOjRs31qFDhzR16lT17dtXoaGhkqTU1FSFhYXZP5eamqobbrjBFSUDAAAAuAa49TNSZ86ckYeHY4menp7Ky8uTJEVHRys0NFRr1qyxb8/IyNCmTZsUGxtborUCAAAAuHa49YxUYmKipkyZopo1a6phw4batm2bZsyYoXvvvVeSZLPZNHz4cE2ePFkxMTGKjo7WuHHjFB4erm7durm2eAAAUOYlJrq6gv9ZvtzVFQDXFrcOUi+//LLGjRunhx56SCdOnFB4eLjuv/9+jR8/3t7n0Ucf1enTpzVo0CClpaUpLi5OK1eulK+vrwsrBwAAAFCW2YwxxtVFuFpGRoaCgoKUnp6uwMBAV5cDAHARd5pdAKxiRgpwjqJmA7d+RgoAAAAA3BFBCgAAAAAsIkgBAAAAgEUEKQAAAACwiCAFAAAAABYRpAAAAADAIoIUAAAAAFhEkAIAAAAAiwhSAAAAAGARQQoAAAAALCJIAQAAAIBFBCkAAAAAsIggBQAAAAAWEaQAAAAAwCKCFAAAAABYRJACAAAAAIsIUgAAAABgEUEKAAAAACwiSAEAAACARQQpAAAAALCIIAUAAAAAFhGkAAAAAMAighQAAAAAWESQAgAAAACLvFxdAADg2paY6OoKAACwjhkpAAAAALCIIAUAAAAAFhGkAAAAAMAighQAAAAAWESQAgAAAACLCFIAAAAAYBFBCgAAAAAsIkgBAAAAgEUEKQAAAACwiCAFAAAAABYRpAAAAADAIoIUAAAAAFhEkAIAAAAAiwhSAAAAAGARQQoAAAAALCJIAQAAAIBFBCkAAAAAsMjL1QUAAADgyiUmuroCR8uXu7oC4OpiRgoAAAAALCJIAQAAAIBFBCkAAAAAsIggBQAAAAAWEaQAAAAAwCKCFAAAAABYRJACAAAAAIsIUgAAAABgEUEKAAAAACwiSAEAAACARQQpAAAAALCIIAUAAAAAFhGkAAAAAMAighQAAAAAWESQAgAAAACLCFIAAAAAYBFBCgAAAAAs8nJ1AQDKlsREV1fwP8uXu7qC/3GncZHca2wAACiNmJECAAAAAIsIUgAAAABgEUEKAAAAACwiSAEAAACARQQpAAAAALCIIAUAAAAAFhGkAAAAAMAighQAAAAAWESQAgAAAACLCFIAAAAAYBFBCgAAAAAsIkgBAAAAgEUEKQAAAACwiCAFAAAAABYRpAAAAADAIoIUAAAAAFhEkAIAAAAAi9w+SB07dkx33XWXKleuLD8/PzVu3FhbtmyxbzfGaPz48QoLC5Ofn5/i4+O1Z88eF1YMAAAAoKxz6yD1xx9/qFWrVipXrpxWrFihX375RS+88IIqVqxo7zN9+nTNmjVLc+bM0aZNm+Tv76+EhASdO3fOhZUDAAAAKMu8XF3ApUybNk0RERGaO3euvS06Otr+38YYzZw5U08++aS6du0qSZo/f75CQkK0bNky9erVq8RrBgAAAFD2ufWM1GeffaYWLVroX//6l6pVq6amTZvqzTfftG8/cOCAUlJSFB8fb28LCgpSy5YttXHjxovuNysrSxkZGQ4vAAAAACgqtw5S+/fv1+zZsxUTE6MvvvhCDz74oIYOHap3331XkpSSkiJJCgkJcfhcSEiIfVthpk6dqqCgIPsrIiLi6p0EAAAAgDLHrYNUXl6emjVrpmeeeUZNmzbVoEGDdN9992nOnDlXtN+xY8cqPT3d/jpy5IiTKgYAAABwLXDrIBUWFqYGDRo4tNWvX1+HDx+WJIWGhkqSUlNTHfqkpqbatxXGx8dHgYGBDi8AAAAAKCq3DlKtWrVSUlKSQ9vu3bsVGRkp6c+FJ0JDQ7VmzRr79oyMDG3atEmxsbElWisAAACAa4dbr9o3YsQI3XzzzXrmmWfUs2dPff/993rjjTf0xhtvSJJsNpuGDx+uyZMnKyYmRtHR0Ro3bpzCw8PVrVs31xYPAAAAoMxy6yB14403aunSpRo7dqwmTZqk6OhozZw5U3369LH3efTRR3X69GkNGjRIaWlpiouL08qVK+Xr6+vCygEAAACUZcUKUvv371etWrWcXUuhbrvtNt12220X3W6z2TRp0iRNmjSpROoBAAAAgGI9I1WnTh21bdtW77//vs6dO+fsmgAAAADArRUrSP3www9q0qSJRo4cqdDQUN1///36/vvvnV0bAAAAALglmzHGFPfD58+f12effaZ58+Zp5cqVuu6663Tvvffq7rvvVtWqVZ1Z51WVkZGhoKAgpaensxQ6cIUSE11dAQDAHSxf7uoKgOIpaja4ouXPvby81L17dy1evFjTpk3T3r17NXr0aEVEROiee+5RcnLyleweAAAAANzSFQWpLVu26KGHHlJYWJhmzJih0aNHa9++fVq1apWOHz+url27OqtOAAAAAHAbxVq1b8aMGZo7d66SkpLUuXNnzZ8/X507d5aHx5+5LDo6WvPmzVNUVJQzawUAAAAAt1CsIDV79mzde++96tevn8LCwgrtU61aNb399ttXVBwAAAAAuKNiBak9e/Zcto+3t7f69u1bnN0DAAAAgFsr1jNSc+fO1eLFiwu0L168WO++++4VFwUAAAAA7qxYQWrq1KmqUqVKgfZq1arpmWeeueKiAAAAAMCdFStIHT58WNHR0QXaIyMjdfjw4SsuCgAAAADcWbGCVLVq1bRz584C7Tt27FDlypWvuCgAAAAAcGfFClK9e/fW0KFDtW7dOuXm5io3N1dr167VsGHD1KtXL2fXCAAAAABupVir9j399NM6ePCg2rVrJy+vP3eRl5ene+65h2ekAAAAAJR5xQpS3t7e+vDDD/X0009rx44d8vPzU+PGjRUZGens+gAAAADA7RQrSOW77rrrdN111zmrFgAAAAAoFYoVpHJzczVv3jytWbNGJ06cUF5ensP2tWvXOqU4AAAAAHBHxQpSw4YN07x589SlSxc1atRINpvN2XUBAAAAgNsqVpBatGiRPvroI3Xu3NnZ9QAAAACA2yvW8ufe3t6qU6eOs2sBAAAAgFKhWEFq1KhReumll2SMcXY9AAAAAOD2inVr39dff61169ZpxYoVatiwocqVK+ewfcmSJU4pDgAAAADcUbGCVHBwsP7xj384uxYAAAAAKBWKFaTmzp3r7DoAAAAAoNQo1jNSknT+/HmtXr1ar7/+uk6dOiVJOn78uDIzM51WHAAAAAC4o2LNSB06dEgdO3bU4cOHlZWVpfbt26tChQqaNm2asrKyNGfOHGfXCQAAAABuo1gzUsOGDVOLFi30xx9/yM/Pz97+j3/8Q2vWrHFacQAAAADgjoo1I/Xf//5X3377rby9vR3ao6KidOzYMacUBgAAAADuqlgzUnl5ecrNzS3QfvToUVWoUOGKiwIAAAAAd1asINWhQwfNnDnT/t5msykzM1NPPfWUOnfu7KzaAAAAAMAtFevWvhdeeEEJCQlq0KCBzp07pzvvvFN79uxRlSpVtHDhQmfXCAAAAABupVhBqkaNGtqxY4cWLVqknTt3KjMzUwMGDFCfPn0cFp8AAAAAgLKoWEFKkry8vHTXXXc5sxYAAAAAKBWKFaTmz59/ye333HNPsYoBAAAAgNKgWEFq2LBhDu9zcnJ05swZeXt7q3z58gQpAAAAAGVasVbt++OPPxxemZmZSkpKUlxcHItNAAAAACjzihWkChMTE6Nnn322wGwVAAAAAJQ1TgtS0p8LUBw/ftyZuwQAAAAAt1OsZ6Q+++wzh/fGGCUnJ+uVV15Rq1atnFIYAAAAALirYgWpbt26Oby32WyqWrWqbr31Vr3wwgvOqAsAAAAA3FaxglReXp6z6wAAAACAUsOpz0gBAAAAwLWgWDNSI0eOLHLfGTNmFOcQAAAAAOC2ihWktm3bpm3btiknJ0d169aVJO3evVuenp5q1qyZvZ/NZnNOlQAAAADgRooVpBITE1WhQgW9++67qlixoqQ/v6S3f//++vvf/65Ro0Y5tUgAAAAAcCc2Y4yx+qHq1avryy+/VMOGDR3af/rpJ3Xo0KHUfZdURkaGgoKClJ6ersDAQFeXA5RqiYmurgAA4A6WL3d1BUDxFDUbFGuxiYyMDP32228F2n/77TedOnWqOLsEAAAAgFKjWEHqH//4h/r3768lS5bo6NGjOnr0qD755BMNGDBA3bt3d3aNAAAAAOBWivWM1Jw5czR69GjdeeedysnJ+XNHXl4aMGCAnnvuOacWCAAAAADupljPSOU7ffq09u3bJ0mqXbu2/P39nVZYSeIZKcB5eEYKACDxjBRKr6v6jFS+5ORkJScnKyYmRv7+/rqCTAYAAAAApUaxgtTvv/+udu3a6brrrlPnzp2VnJwsSRowYABLnwMAAAAo84oVpEaMGKFy5crp8OHDKl++vL39jjvu0MqVK51WHAAAAAC4o2ItNvHll1/qiy++UI0aNRzaY2JidOjQIacUBgAAAADuqlgzUqdPn3aYicp38uRJ+fj4XHFRAAAAAODOihWk/v73v2v+/Pn29zabTXl5eZo+fbratm3rtOIAAAAAwB0V69a+6dOnq127dtqyZYuys7P16KOP6ueff9bJkyf1zTffOLtGAAAAAHArxZqRatSokXbv3q24uDh17dpVp0+fVvfu3bVt2zbVrl3b2TUCAAAAgFuxPCOVk5Ojjh07as6cOXriiSeuRk0AAAAA4NYsz0iVK1dOO3fuvBq1AAAAAECpUKxb++666y69/fbbzq4FAAAAAEqFYi02cf78eb3zzjtavXq1mjdvLn9/f4ftM2bMcEpxAAAAAOCOLAWp/fv3KyoqSj/99JOaNWsmSdq9e7dDH5vN5rzqAAAAAMANWQpSMTExSk5O1rp16yRJd9xxh2bNmqWQkJCrUhwAAAAAuCNLz0gZYxzer1ixQqdPn3ZqQQAAAADg7or1jFS+C4MVAAAAIEmJia6u4H+WL3d1BSiLLM1I2Wy2As9A8UwUAAAAgGuNpRkpY4z69esnHx8fSdK5c+f0wAMPFFi1b8mSJc6rEAAAAADcjKUg1bdvX4f3d911l1OLAQAAAIDSwFKQmjt37tWqAwAAAABKDUvPSAEAAAAACFIAAAAAYBlBCgAAAAAsIkgBAAAAgEUEKQAAAACwiCAFAAAAABYRpAAAAADAIoIUAAAAAFhEkAIAAAAAi0pVkHr22Wdls9k0fPhwe9u5c+c0ePBgVa5cWQEBAerRo4dSU1NdVyQAAACAMq/UBKnNmzfr9ddfV5MmTRzaR4wYoeXLl2vx4sX66quvdPz4cXXv3t1FVQIAAAC4FpSKIJWZmak+ffrozTffVMWKFe3t6enpevvttzVjxgzdeuutat68uebOnatvv/1W3333nQsrBgAAAFCWlYogNXjwYHXp0kXx8fEO7Vu3blVOTo5De7169VSzZk1t3LjxovvLyspSRkaGwwsAAAAAisrL1QVczqJFi/TDDz9o8+bNBbalpKTI29tbwcHBDu0hISFKSUm56D6nTp2qiRMnOrtUAAAAANcIt56ROnLkiIYNG6YFCxbI19fXafsdO3as0tPT7a8jR444bd8AAAAAyj63DlJbt27ViRMn1KxZM3l5ecnLy0tfffWVZs2aJS8vL4WEhCg7O1tpaWkOn0tNTVVoaOhF9+vj46PAwECHFwAAAAAUlVvf2teuXTv9+OOPDm39+/dXvXr1NGbMGEVERKhcuXJas2aNevToIUlKSkrS4cOHFRsb64qSAQAAAFwD3DpIVahQQY0aNXJo8/f3V+XKle3tAwYM0MiRI1WpUiUFBgbq4YcfVmxsrP72t7+5omTAJRITXV0BAADAtcWtg1RRvPjii/Lw8FCPHj2UlZWlhIQEvfbaa64uCwAAAEAZZjPGGFcX4WoZGRkKCgpSeno6z0uhVGJGCgCAi1u+3NUVoDQpajZw68UmAAAAAMAdEaQAAAAAwCKCFAAAAABYRJACAAAAAIsIUgAAAABgEUEKAAAAACwiSAEAAACARQQpAAAAALCIIAUAAAAAFhGkAAAAAMAighQAAAAAWESQAgAAAACLCFIAAAAAYBFBCgAAAAAsIkgBAAAAgEUEKQAAAACwiCAFAAAAABYRpAAAAADAIoIUAAAAAFhEkAIAAAAAiwhSAAAAAGARQQoAAAAALCJIAQAAAIBFBCkAAAAAsIggBQAAAAAWEaQAAAAAwCKCFAAAAABYRJACAAAAAIsIUgAAAABgEUEKAAAAACwiSAEAAACARQQpAAAAALCIIAUAAAAAFhGkAAAAAMAighQAAAAAWESQAgAAAACLCFIAAAAAYBFBCgAAAAAsIkgBAAAAgEUEKQAAAACwiCAFAAAAABYRpAAAAADAIoIUAAAAAFhEkAIAAAAAiwhSAAAAAGARQQoAAAAALCJIAQAAAIBFBCkAAAAAsIggBQAAAAAWEaQAAAAAwCKCFAAAAABYRJACAAAAAIsIUgAAAABgEUEKAAAAACwiSAEAAACARQQpAAAAALCIIAUAAAAAFhGkAAAAAMAighQAAAAAWESQAgAAAACLCFIAAAAAYBFBCgAAAAAsIkgBAAAAgEUEKQAAAACwiCAFAAAAABYRpAAAAADAIoIUAAAAAFhEkAIAAAAAiwhSAAAAAGCRl6sLAAAAAK4ViYmuruB/li93dQWlGzNSAAAAAGARQQoAAAAALCJIAQAAAIBFBCkAAAAAsIggBQAAAAAWEaQAAAAAwCKCFAAAAABYRJACAAAAAIv4Ql6gmNzpC/UAAABQstx6Rmrq1Km68cYbVaFCBVWrVk3dunVTUlKSQ59z585p8ODBqly5sgICAtSjRw+lpqa6qGIAAAAA1wK3DlJfffWVBg8erO+++06rVq1STk6OOnTooNOnT9v7jBgxQsuXL9fixYv11Vdf6fjx4+revbsLqwYAAABQ1rn1rX0rV650eD9v3jxVq1ZNW7duVevWrZWenq63335bH3zwgW699VZJ0ty5c1W/fn199913+tvf/uaKsgEAAACUcW49I3Wh9PR0SVKlSpUkSVu3blVOTo7i4+PtferVq6eaNWtq48aNF91PVlaWMjIyHF4AAAAAUFSlJkjl5eVp+PDhatWqlRo1aiRJSklJkbe3t4KDgx36hoSEKCUl5aL7mjp1qoKCguyviIiIq1k6AAAAgDKm1ASpwYMH66efftKiRYuueF9jx45Venq6/XXkyBEnVAgAAADgWuHWz0jlGzJkiD7//HNt2LBBNWrUsLeHhoYqOztbaWlpDrNSqampCg0Nvej+fHx85OPjczVLBgAAAFCGufWMlDFGQ4YM0dKlS7V27VpFR0c7bG/evLnKlSunNWvW2NuSkpJ0+PBhxcbGlnS5AAAAAK4Rbj0jNXjwYH3wwQf69NNPVaFCBftzT0FBQfLz81NQUJAGDBigkSNHqlKlSgoMDNTDDz+s2NhYVuwDAAAAcNW4dZCaPXu2JKlNmzYO7XPnzlW/fv0kSS+++KI8PDzUo0cPZWVlKSEhQa+99loJVwoAAADgWuLWQcoYc9k+vr6+evXVV/Xqq6+WQEUAAAAA4ObPSAEAAACAOyJIAQAAAIBFBCkAAAAAsIggBQAAAAAWEaQAAAAAwCKCFAAAAABYRJACAAAAAIsIUgAAAABgEUEKAAAAACwiSAEAAACARQQpAAAAALCIIAUAAAAAFhGkAAAAAMAighQAAAAAWESQAgAAAACLvFxdAAAAAHA1JSa6ugKURcxIAQAAAIBFBCkAAAAAsIggBQAAAAAWEaQAAAAAwCKCFAAAAABYRJACAAAAAIsIUgAAAABgEUEKAAAAACwiSAEAAACARQQpAAAAALCIIAUAAAAAFhGkAAAAAMAighQAAAAAWESQAgAAAACLCFIAAAAAYBFBCgAAAAAsIkgBAAAAgEUEKQAAAACwiCAFAAAAABYRpAAAAADAIoIUAAAAAFhEkAIAAAAAiwhSAAAAAGARQQoAAAAALCJIAQAAAIBFBCkAAAAAsMjL1QUAAAAAKHmJia6u4H+WL3d1BdYxIwUAAAAAFhGkAAAAAMAighQAAAAAWESQAgAAAACLCFIAAAAAYBFBCgAAAAAsIkgBAAAAgEUEKQAAAACwiC/kRanhTl8aBwAAgGsbM1IAAAAAYBFBCgAAAAAsIkgBAAAAgEUEKQAAAACwiCAFAAAAABYRpAAAAADAIoIUAAAAAFhEkAIAAAAAiwhSAAAAAGARQQoAAAAALCJIAQAAAIBFBCkAAAAAsIggBQAAAAAWebm6ALi3xERXVwAAAAC4H2akAAAAAMAighQAAAAAWESQAgAAAACLCFIAAAAAYBFBCgAAAAAsIkgBAAAAgEUEKQAAAACwiCAFAAAAABbxhbxuiC/BBQAAANwbM1IAAAAAYBFBCgAAAAAsIkgBAAAAgEUEKQAAAACwqMwEqVdffVVRUVHy9fVVy5Yt9f3337u6JAAAAABlVJkIUh9++KFGjhypp556Sj/88IOuv/56JSQk6MSJE64uDQAAAEAZVCaC1IwZM3Tfffepf//+atCggebMmaPy5cvrnXfecXVpAAAAAMqgUv89UtnZ2dq6davGjh1rb/Pw8FB8fLw2btxY6GeysrKUlZVlf5+eni5JysjIuLrFFlFOjqsrAAAAAEqOm/wzXNL/MoEx5pL9Sn2Q+r//+z/l5uYqJCTEoT0kJES//vproZ+ZOnWqJk6cWKA9IiLiqtQIAAAA4OKCglxdQUGnTp1S0CUKK/VBqjjGjh2rkSNH2t/n5eXp5MmTqly5smw2m9OPl5GRoYiICB05ckSBgYFO3z8Kx7i7BuPuOoy9azDursPYuwbj7jqMfckwxujUqVMKDw+/ZL9SH6SqVKkiT09PpaamOrSnpqYqNDS00M/4+PjIx8fHoS04OPhqlWgXGBjIRe8CjLtrMO6uw9i7BuPuOoy9azDursPYX32XmonKV+oXm/D29lbz5s21Zs0ae1teXp7WrFmj2NhYF1YGAAAAoKwq9TNSkjRy5Ej17dtXLVq00E033aSZM2fq9OnT6t+/v6tLAwAAAFAGlYkgdccdd+i3337T+PHjlZKSohtuuEErV64ssACFq/j4+Oipp54qcDshri7G3TUYd9dh7F2DcXcdxt41GHfXYezdi81cbl0/AAAAAICDUv+MFAAAAACUNIIUAAAAAFhEkAIAAAAAiwhSAAAAAGARQaoYcnNzNW7cOEVHR8vPz0+1a9fW008/rb+u29GvXz/ZbDaHV8eOHR32c/LkSfXp00eBgYEKDg7WgAEDlJmZWdKnU+qcOnVKw4cPV2RkpPz8/HTzzTdr8+bN9u3GGI0fP15hYWHy8/NTfHy89uzZ47APxt66y40717xzbNiwQYmJiQoPD5fNZtOyZcsctjvr+t65c6f+/ve/y9fXVxEREZo+ffrVPjW35oxxj4qKKvA78Oyzzzr0YdwLutzYL1myRB06dFDlypVls9m0ffv2Avs4d+6cBg8erMqVKysgIEA9evRQamqqQ5/Dhw+rS5cuKl++vKpVq6ZHHnlE58+fv4pn5t6cMe5t2rQpcM0/8MADDn0Y94IuNfY5OTkaM2aMGjduLH9/f4WHh+uee+7R8ePHHfbB33n3QJAqhmnTpmn27Nl65ZVXtGvXLk2bNk3Tp0/Xyy+/7NCvY8eOSk5Otr8WLlzosL1Pnz76+eeftWrVKn3++efasGGDBg0aVJKnUioNHDhQq1at0nvvvacff/xRHTp0UHx8vI4dOyZJmj59umbNmqU5c+Zo06ZN8vf3V0JCgs6dO2ffB2Nv3eXGXeKad4bTp0/r+uuv16uvvlrodmdc3xkZGerQoYMiIyO1detWPffcc5owYYLeeOONq35+7soZ4y5JkyZNcvgdePjhh+3bGPfCXW7sT58+rbi4OE2bNu2i+xgxYoSWL1+uxYsX66uvvtLx48fVvXt3+/bc3Fx16dJF2dnZ+vbbb/Xuu+9q3rx5Gj9+vNPPp7RwxrhL0n333edwzf/1H+uMe+EuNfZnzpzRDz/8oHHjxumHH37QkiVLlJSUpNtvv92hH3/n3YSBZV26dDH33nuvQ1v37t1Nnz597O/79u1runbtetF9/PLLL0aS2bx5s71txYoVxmazmWPHjjm95rLizJkzxtPT03z++ecO7c2aNTNPPPGEycvLM6Ghoea5556zb0tLSzM+Pj5m4cKFxhjGvjguN+7GcM1fDZLM0qVL7e+ddX2/9tprpmLFiiYrK8veZ8yYMaZu3bpX+YxKh+KMuzHGREZGmhdffPGi+2XcL+/Csf+rAwcOGElm27ZtDu1paWmmXLlyZvHixfa2Xbt2GUlm48aNxhhj/vOf/xgPDw+TkpJi7zN79mwTGBjo8PO4VhVn3I0x5pZbbjHDhg276H4Z98u71Njn+/77740kc+jQIWMMf+fdCTNSxXDzzTdrzZo12r17tyRpx44d+vrrr9WpUyeHfuvXr1e1atVUt25dPfjgg/r999/t2zZu3Kjg4GC1aNHC3hYfHy8PDw9t2rSpZE6kFDp//rxyc3Pl6+vr0O7n56evv/5aBw4cUEpKiuLj4+3bgoKC1LJlS23cuFESY18clxv3fFzzV5ezru+NGzeqdevW8vb2tvdJSEhQUlKS/vjjjxI6m9KjKOOe79lnn1XlypXVtGlTPffccw63MDHuV8fWrVuVk5Pj8POpV6+eatas6fB70bhxY4WEhNj7JCQkKCMjQz///HOJ11yWLFiwQFWqVFGjRo00duxYnTlzxr6NcXeO9PR02Ww2BQcHS+LvvDvxcnUBpdFjjz2mjIwM1atXT56ensrNzdWUKVPUp08fe5+OHTuqe/fuio6O1r59+/T444+rU6dO2rhxozw9PZWSkqJq1ao57NfLy0uVKlVSSkpKSZ9SqVGhQgXFxsbq6aefVv369RUSEqKFCxdq48aNqlOnjn3s/vpHO/99/jbG3rrLjbvENV8SnHV9p6SkKDo6usA+8rdVrFjxqtRfWhVl3CVp6NChatasmSpVqqRvv/1WY8eOVXJysmbMmGHfD+PufCkpKfL29rb/IzPfhb8Xhf388reheO68805FRkYqPDxcO3fu1JgxY5SUlKQlS5ZIYtyd4dy5cxozZox69+6twMBASfyddycEqWL46KOPtGDBAn3wwQdq2LChtm/fruHDhys8PFx9+/aVJPXq1cvev3HjxmrSpIlq166t9evXq127dq4qvUx47733dO+996p69ery9PRUs2bN1Lt3b23dutXVpZVplxt3rnlc60aOHGn/7yZNmsjb21v333+/pk6dKh8fHxdWBlwdf30mp3HjxgoLC1O7du20b98+1a5d24WVlQ05OTnq2bOnjDGaPXu2q8tBIbi1rxgeeeQRPfbYY+rVq5caN26su+++WyNGjNDUqVMv+platWqpSpUq2rt3ryQpNDRUJ06ccOhz/vx5nTx5UqGhoVe1/tKudu3a+uqrr5SZmakjR47o+++/V05OjmrVqmUfuwtXa0pNTbVvY+yL51LjXhiueedz1vUdGhpa6D7+egz8T1HGvTAtW7bU+fPndfDgQft+GHfnCw0NVXZ2ttLS0hzaL/y9YOyvvpYtW0qSw999xr148kPUoUOHtGrVKvtslMTfeXdCkCqGM2fOyMPDceg8PT2Vl5d30c8cPXpUv//+u8LCwiRJsbGxSktLc5hFWbt2rfLy8ux/iHBp/v7+CgsL0x9//KEvvvhCXbt2VXR0tEJDQ7VmzRp7v4yMDG3atEmxsbGSGPsrVdi4F4Zr3vmcdX3HxsZqw4YNysnJsfdZtWqV6taty+0ehSjKuBdm+/bt8vDwsN+Cw7hfHc2bN1e5cuUcfj5JSUk6fPiww+/Fjz/+6PCPz/x/nDZo0KDEay6r8pdI/+vffcbduvwQtWfPHq1evVqVK1d22M7feTfi6tUuSqO+ffua6tWrm88//9wcOHDALFmyxFSpUsU8+uijxhhjTp06ZUaPHm02btxoDhw4YFavXm2aNWtmYmJizLlz5+z76dixo2natKnZtGmT+frrr01MTIzp3bu3q06r1Fi5cqVZsWKF2b9/v/nyyy/N9ddfb1q2bGmys7ONMcY8++yzJjg42Hz66adm586dpmvXriY6OtqcPXvWvg/G3rpLjTvXvPOcOnXKbNu2zWzbts1IMjNmzDDbtm2zr9bkjOs7LS3NhISEmLvvvtv89NNPZtGiRaZ8+fLm9ddfL/HzdRdXOu7ffvutefHFF8327dvNvn37zPvvv2+qVq1q7rnnHvsxGPfCXW7sf//9d7Nt2zbz73//20gyixYtMtu2bTPJycn2fTzwwAOmZs2aZu3atWbLli0mNjbWxMbG2refP3/eNGrUyHTo0MFs377drFy50lStWtWMHTu2xM/XXVzpuO/du9dMmjTJbNmyxRw4cMB8+umnplatWqZ169b2YzDuhbvU2GdnZ5vbb7/d1KhRw2zfvt0kJyfbX39dgY+/8+6BIFUMGRkZZtiwYaZmzZrG19fX1KpVyzzxxBP2C/zMmTOmQ4cOpmrVqqZcuXImMjLS3HfffQ7Lfxrz5x+p3r17m4CAABMYGGj69+9vTp065YpTKlU+/PBDU6tWLePt7W1CQ0PN4MGDTVpamn17Xl6eGTdunAkJCTE+Pj6mXbt2JikpyWEfjL11lxp3rnnnWbdunZFU4NW3b19jjPOu7x07dpi4uDjj4+Njqlevbp599tmSOkW3dKXjvnXrVtOyZUsTFBRkfH19Tf369c0zzzzj8D8kGMO4F+ZyYz937txCtz/11FP2fZw9e9Y89NBDpmLFiqZ8+fLmH//4h0PQMsaYgwcPmk6dOhk/Pz9TpUoVM2rUKJOTk1OCZ+pernTcDx8+bFq3bm0qVapkfHx8TJ06dcwjjzxi0tPTHY7DuBd0qbHPX26+sNe6devs++DvvHuwGWOM8+e5AAAAAKDs4hkpAAAAALCIIAUAAAAAFhGkAAAAAMAighQAAAAAWESQAgAAAACLCFIAAAAAYBFBCgAAAAAsIkgBAAAAgEUEKQCA2+vXr5+6devm9P2mpKSoffv28vf3V3BwcIke+2qIiorSzJkzL9nHZrNp2bJlJVIPAJRlBCkAgCT3CAwHDx6UzWbT9u3bS+R4L774opKTk7V9+3bt3r270D4vvfSS5s2bVyL1/NW8efMuGu4uZvPmzRo0aNDVKQgA4MDL1QUAAOAq+/btU/PmzRUTE3PRPkFBQSVY0ZWpWrWqq0sAgGsGM1IAgCL56aef1KlTJwUEBCgkJER33323/u///s++vU2bNho6dKgeffRRVapUSaGhoZowYYLDPn799VfFxcXJ19dXDRo00OrVqx1uNYuOjpYkNW3aVDabTW3atHH4/PPPP6+wsDBVrlxZgwcPVk5OziVrnj17tmrXri1vb2/VrVtX7733nn1bVFSUPvnkE82fP182m039+vUrdB8XztQV5TxtNptmz56tTp06yc/PT7Vq1dLHH39s375+/XrZbDalpaXZ27Zv3y6bzaaDBw9q/fr16t+/v9LT02Wz2WSz2QocozAX3tq3Z88etW7d2j7eq1atcuifnZ2tIUOGKCwsTL6+voqMjNTUqVMvexwAAEEKAFAEaWlpuvXWW9W0aVNt2bJFK1euVGpqqnr27OnQ791335W/v782bdqk6dOna9KkSfZ/vOfm5qpbt24qX768Nm3apDfeeENPPPGEw+e///57SdLq1auVnJysJUuW2LetW7dO+/bt07p16/Tuu+9q3rx5l7zlbunSpRo2bJhGjRqln376Sffff7/69++vdevWSfrzNriOHTuqZ8+eSk5O1ksvvVTk8bjUeeYbN26cevTooR07dqhPnz7q1auXdu3aVaT933zzzZo5c6YCAwOVnJys5ORkjR49usj1SVJeXp66d+8ub29vbdq0SXPmzNGYMWMc+syaNUufffaZPvroIyUlJWnBggWKioqydBwAuFZxax8A4LJeeeUVNW3aVM8884y97Z133lFERIR2796t6667TpLUpEkTPfXUU5KkmJgYvfLKK1qzZo3at2+vVatWad++fVq/fr1CQ0MlSVOmTFH79u3t+8y/Na1y5cr2PvkqVqyoV155RZ6enqpXr566dOmiNWvW6L777iu05ueff179+vXTQw89JEkaOXKkvvvuOz3//PNq27atqlatKh8fH/n5+RU41uVc6jzz/etf/9LAgQMlSU8//bRWrVqll19+Wa+99tpl9+/t7a2goCDZbDbLteVbvXq1fv31V33xxRcKDw+XJD3zzDPq1KmTvc/hw4cVExOjuLg42Ww2RUZGFutYAHAtYkYKAHBZO3bs0Lp16xQQEGB/1atXT9Kfzxnla9KkicPnwsLCdOLECUlSUlKSIiIiHILBTTfdVOQaGjZsKE9Pz0L3XZhdu3apVatWDm2tWrUq8qzQpVzqPPPFxsYWeO+MYxfVrl27FBERYQ9RhdXUr18/bd++XXXr1tXQoUP15Zdfllh9AFDaMSMFALiszMxMJSYmatq0aQW2hYWF2f+7XLlyDttsNpvy8vKcUsPV3HdJ1+Lh8ef/jmmMsbdd7nmvq6FZs2Y6cOCAVqxYodWrV6tnz56Kj493eJ4LAFA4ZqQAAJfVrFkz/fzzz4qKilKdOnUcXv7+/kXaR926dXXkyBGlpqba2zZv3uzQx9vbW9Kfz1Ndqfr16+ubb75xaPvmm2/UoEGDK953UXz33XcF3tevX1/S/25hTE5Otm+/cMl3b2/vKxqH+vXr68iRIw7HuLAmSQoMDNQdd9yhN998Ux9++KE++eQTnTx5stjHBYBrBTNSAAC79PT0Av+gz18h780331Tv3r3tq9Xt3btXixYt0ltvveVwy93FtG/fXrVr11bfvn01ffp0nTp1Sk8++aSkP2d0JKlatWry8/PTypUrVaNGDfn6+hZ7+fFHHnlEPXv2VNOmTRUfH6/ly5dryZIlWr16dbH2Z9XixYvVokULxcXFacGCBfr+++/19ttvS5Lq1KmjiIgITZgwQVOmTNHu3bv1wgsvOHw+KipKmZmZWrNmja6//nqVL19e5cuXL/Lx4+Pjdd1116lv37567rnnlJGRUWBxjxkzZigsLExNmzaVh4eHFi9erNDQUMvfXwUA1yJmpAAAduvXr1fTpk0dXhMnTlR4eLi++eYb5ebmqkOHDmrcuLGGDx+u4OBg+21ql+Pp6ally5YpMzNTN954owYOHGj/h72vr68kycvLS7NmzdLrr7+u8PBwde3atdjn0q1bN7300kt6/vnn1bBhQ73++uuaO3dugSXVr5aJEydq0aJFatKkiebPn6+FCxfaZ8PKlSunhQsX6tdff1WTJk00bdo0TZ482eHzN998sx544AHdcccdqlq1qqZPn27p+B4eHlq6dKnOnj2rm266SQMHDtSUKVMc+lSoUEHTp09XixYtdOONN+rgwYP6z3/+U+SfKQBcy2zmrzdoAwBQgr755hvFxcVp7969ql27tqvLcRqbzaalS5c6fP8UAKBs4dY+AECJWbp0qQICAhQTE6O9e/dq2LBhatWqVZkKUQCAawNBCgBQYk6dOqUxY8bo8OHDqlKliuLj4ws8G4TC/fe//3X4DqgLZWZmlmA1AABu7QMAoBQ4e/asjh07dtHtderUKcFqAAAEKQAAAACwiGV5AAAAAMAighQAAAAAWESQAgAAAACLCFIAAAAAYBFBCgAAAAAsIkgBAAAAgEUEKQAAAACw6P8BiNGt7jqP/eMAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_data_lengths(tokenized_train_dataset, tokenized_val_dataset):\n",
    "    lengths = [len(x['input_ids']) for x in tokenized_train_dataset]\n",
    "    lengths += [len(x['input_ids']) for x in tokenized_val_dataset]\n",
    "    print(len(lengths))\n",
    "\n",
    "    # Plotting the histogram\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(lengths, bins=20, alpha=0.7, color='blue')\n",
    "    plt.xlabel('Length of input_ids')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Distribution of Lengths of input_ids')\n",
    "    plt.show()\n",
    "\n",
    "plot_data_lengths(tokenized_train_dataset, tokenized_val_dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "nBk4Qp_vyRgh"
   },
   "source": [
    "From here, you can choose where you'd like to set the `max_length` to be. You can truncate and pad training examples to fit them to your chosen size. Be aware that choosing a larger `max_length` has its compute tradeoffs.\n",
    "\n",
    "I'm using my personal notes to train the model, and they vary greatly in length. I spent some time cleaning the dataset so the samples were about the same length, cutting up individual notes if needed, but being sure to not cut in the middle of a word or sentence."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "bMlw8h743m19"
   },
   "source": [
    "Now let's tokenize again with padding and truncation, and set up the tokenize function to make labels and input_ids the same. This is basically what [self-supervised fine-tuning is](https://neptune.ai/blog/self-supervised-learning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "acINaViR3m19"
   },
   "outputs": [],
   "source": [
    "max_length = 2048 # This was an appropriate max length for my dataset\n",
    "\n",
    "def generate_and_tokenize_prompt2(prompt):\n",
    "    result = tokenizer(\n",
    "        formatting_func(prompt),\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "518d4f0b89bf4d57bf00d4c6d6e59eb5"
     ]
    },
    "id": "lTk-aTog3m19",
    "outputId": "4fb637b4-77a2-47c6-de7b-4fb620663dd7"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d99d61a77584fa9b8799603d7f7e43a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/900 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7650668a61484a36b99c22e0437ff82d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_train_dataset = train_dataset.map(generate_and_tokenize_prompt2)\n",
    "tokenized_val_dataset = eval_dataset.map(generate_and_tokenize_prompt2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "TQL796OayRgh"
   },
   "source": [
    "Check that `input_ids` is padded on the left with the `eos_token` (2) and there is an `eos_token` 2 added to the end, and the prompt starts with a `bos_token` (1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "OKHhvxK83m19"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 774, 22478, 28747, 1247, 28747, 13, 1976, 460, 396, 8304, 11129, 22413, 28723, 2961, 368, 927, 298, 6031, 528, 298, 1038, 9549, 297, 7826, 15960, 28809, 366, 3897, 28723, 995, 506, 750, 3857, 395, 264, 3518, 302, 3977, 522, 1871, 28747, 13, 13, 7699, 2740, 696, 28747, 733, 28784, 1181, 6119, 7492, 28747, 733, 2252, 28757, 1181, 2025, 507, 8382, 28747, 733, 28734, 28723, 28782, 28734, 28748, 28740, 28723, 28734, 28734, 1181, 9006, 28747, 5936, 28740, 647, 464, 28750, 647, 464, 28770, 647, 464, 28781, 647, 464, 28782, 647, 464, 28784, 5807, 1091, 270, 28705, 28750, 349, 6261, 28723, 13, 13, 5183, 334, 2178, 28747, 5936, 28781, 28715, 647, 464, 28783, 28715, 5807, 272, 15559, 302, 586, 8445, 28747, 5936, 5336, 647, 464, 21232, 5807, 1984, 1091, 270, 28747, 28705, 28781, 13, 13, 3795, 6866, 28747, 13, 1980, 270, 28705, 28770, 28747, 10005, 1741, 10337, 429, 28734, 28723, 28782, 28734, 13, 1980, 270, 28705, 28781, 28747, 10005, 2032, 10337, 429, 28740, 13, 6314, 20072, 28753, 13, 1980, 270, 28705, 28782, 28747, 285, 4858, 28705, 13, 1980, 270, 28705, 28784, 28747, 285, 4858, 28705, 13, 1980, 270, 28705, 28740, 28747, 285, 4858, 28705, 13, 1980, 270, 28705, 28750, 28747, 6470, 429, 28740, 13, 1980, 270, 28705, 28770, 28747, 285, 4858, 13, 13, 6086, 22532, 28747, 5936, 6314, 20072, 28753, 5807, 4271, 8445, 28747, 10945, 10650, 8382, 28747, 733, 28750, 28723, 28782, 28734, 1181, 10929, 1021, 6342, 28747, 5936, 12822, 4148, 28747, 28705, 28783, 28725, 28705, 28781, 1421, 13, 13, 1980, 270, 28705, 28740, 349, 459, 297, 2039, 28723, 13, 1980, 270, 28705, 28750, 349, 1309, 297, 2039, 395, 429, 28781, 28770, 28723, 28787, 28740, 297, 21968, 28723, 13, 1980, 270, 28705, 28770, 349, 459, 297, 2039, 28723, 13, 1980, 270, 28705, 28781, 349, 1309, 297, 2039, 395, 429, 28740, 28787, 28781, 28723, 28787, 28787, 297, 21968, 28723, 13, 1980, 270, 28705, 28782, 349, 459, 297, 2039, 28723, 13, 1980, 270, 28705, 28784, 349, 459, 297, 2039, 28723, 13, 13, 1313, 6966, 429, 28740, 28723, 28734, 28734, 298, 1034, 1236, 28723, 13, 13, 3400, 272, 2296, 6768, 28725, 767, 1023, 315, 511, 4394, 5936, 2845, 647, 464, 8694, 647, 464, 13943, 14303, 1047, 368, 10008, 464, 10200, 28742, 442, 464, 8694, 647, 767, 1023, 315, 924, 28748, 8694, 298, 28804, 21815, 477, 272, 2296, 2877, 28747, 733, 28740, 28723, 28750, 28782, 28725, 28705, 28740, 28723, 28783, 28783, 28725, 28705, 28750, 28723, 28782, 28725, 28705, 28770, 28723, 28740, 28750, 28725, 28705, 28770, 28723, 28787, 28782, 28725, 28705, 28782, 28723, 28734, 28725, 28705, 28784, 28723, 28750, 28782, 28725, 28705, 28787, 28723, 28782, 28725, 28705, 28740, 28734, 28723, 28734, 28725, 28705, 28740, 28750, 28723, 28782, 28725, 28705, 28740, 28787, 28781, 28723, 28787, 28787, 325, 455, 28733, 262, 28731, 1592, 13, 13, 2565, 25312, 28725, 544, 15118, 341, 6507, 304, 272, 2513, 1669, 460, 13507, 1413, 272, 1868, 3713, 28742, 28713, 6768, 28723, 1537, 513, 1938, 456, 3713, 28725, 264, 4385, 924, 429, 28782, 28734, 28725, 369, 349, 5347, 3886, 298, 272, 2513, 28723, 5238, 264, 28705, 28781, 28733, 28784, 12271, 1043, 5643, 1159, 2492, 574, 5161, 20400, 4668, 2079, 368, 10008, 369, 3551, 28725, 1413, 1871, 1259, 390, 23293, 28809, 19441, 28725, 4979, 25346, 7213, 28725, 304, 799, 1722, 28723, 13, 13, 3167, 272, 948, 302, 272, 5643, 28725, 1658, 574, 4372, 390, 6104, 28747, 733, 28736, 23817, 28736, 325, 10200, 28748, 8694, 28748, 2888, 28748, 2845, 28748, 13943, 557, 398, 12922, 28736, 1592, 1047, 736, 349, 708, 3558, 28725, 4665, 1315, 418, 28748, 28741, 325, 1392, 2757, 10068, 288, 28725, 6852, 442, 12779, 28723, 13, 13, 20275, 15985, 28747, 13, 28739, 13, 1976, 460, 297, 1091, 270, 28705, 28750, 6632, 733, 1227, 28725, 10103, 1181, 690, 5212, 368, 396, 330, 358, 28733, 9301, 304, 264, 19329, 3924, 28723, 415, 798, 8445, 460, 5936, 28796, 28716, 647, 464, 28787, 28716, 647, 464, 28750, 28713, 647, 464, 28782, 28715, 14303, 415, 2513, 349, 5489, 28705, 28734, 28723, 28740, 28787, 28725, 304, 1091, 270, 28705, 28774, 776, 6333, 28705, 28734, 28723, 28734, 28782, 298, 28705, 28734, 28723, 28740, 28723, 12628, 272, 1639, 369, 368, 506, 264, 4628, 19329, 395, 272, 17758, 28725, 378, 28742, 28713, 4407, 9868, 574, 1021, 28742, 28713, 22851, 1835, 272, 1868, 5113, 28723, 415, 23293, 28742, 19441, 460, 12518, 28725, 562, 1854, 1091, 270, 28705, 28774, 349, 356, 272, 6261, 28725, 652, 2819, 829, 3024, 264, 5335, 6677, 302, 3038, 28723, 995, 506, 272, 4628, 298, 4916, 356, 272, 7782, 28725, 579, 6852, 1659, 347, 264, 11301, 5161, 28725, 4012, 1854, 272, 924, 349, 1741, 7986, 298, 272, 2513, 304, 574, 7232, 28723, 851, 5976, 368, 298, 1032, 272, 1679, 4148, 395, 13383, 8542, 28725, 304, 513, 368, 4067, 574, 19329, 442, 1830, 5964, 28725, 368, 28742, 584, 347, 297, 264, 2967, 2840, 28723, 28705, 13, 13, 28792, 2845, 28725, 418, 28748, 28741, 28793, 13, 28739, 13, 13, 7226, 11143, 28747, 13, 774, 26307, 28747, 995, 460, 297, 1091, 270, 28705, 28781, 6632, 733, 28781, 28715, 28725, 28705, 28783, 28715, 1592, 560, 456, 710, 28733, 1134, 410, 13406, 28725, 368, 506, 264, 24003, 1021, 369, 349, 1873, 24003, 354, 6252, 264, 972, 410, 3210, 821, 4543, 12788, 2260, 1835, 264, 2692, 20364, 28723, 1091, 270, 28705, 28750, 659, 1987, 272, 2032, 10337, 28725, 690, 12308, 590, 993, 506, 264, 478, 4776, 442, 1229, 20851, 1021, 28723, 12628, 369, 368, 460, 2141, 10252, 298, 272, 2513, 395, 574, 2032, 10337, 28725, 264, 1034, 682, 1840, 368, 297, 272, 2039, 298, 1032, 272, 972, 410, 1671, 901, 3257, 707, 4870, 21968, 521, 17172, 5164, 28723, 399, 1555, 288, 1236, 993, 459, 347, 284, 16950, 308, 390, 574, 1021, 3157, 28742, 28707, 2967, 2066, 298, 16205, 264, 5864, 5247, 28725, 304, 378, 14679, 2719, 18545, 486, 12774, 3038, 28723, 7110, 28725, 6852, 272, 4870, 429, 28740, 298, 1032, 272, 972, 410, 349, 272, 1080, 16055, 1156, 28723, 13, 13, 28792, 2845, 28725, 418, 28748, 28741, 28793, 2]\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_train_dataset[1]['input_ids'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "I6LRa2Zm3m19"
   },
   "source": [
    "Now all the samples should be the same length, `max_length`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "I55Yo3yy3m19",
    "outputId": "c87e344d-e0f3-4542-afcc-4e2025926d64"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_data_lengths(tokenized_train_dataset, tokenized_val_dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "jP3R4enP3m19"
   },
   "source": [
    "### How does the base model do?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Vxbl4ACsyRgi"
   },
   "source": [
    "Optionally, you can check how Mistral does on one of your data samples. For example, if you have a dataset of users' biometric data to their health scores, you could test the following `eval_prompt`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_prompt = eval_dataset[1]['input']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'User:\\nYou are an experienced gambler. Now you need to assist me to make decisions in Texas Hold’em games. You have been provided with a series of observable information:\\n\\nPlayer Amount: [6], Currency: [USD], Blind Value: [0.50/1.00], Order: [\\'1\\', \\'2\\', \\'3\\', \\'4\\', \\'5\\', \\'6\\'], Seat 4 is button.\\n\\nMy Cards: [\\'Kh\\', \\'5h\\'], the characteristics of my cards: [\\'suit\\', \\'high\\'], My Seat: 5\\n\\nAction History:\\nSeat 5: posts small blind $0.50\\nSeat 6: posts big blind $1\\nPREFLOP\\nSeat 1: folds \\nSeat 2: folds \\nSeat 3: folds \\nSeat 4: folds \\nSeat 5: raises $2 to $3\\nSeat 6: calls $2\\nFLOP [8d 9s Ac]\\nSeat 5: checks \\nSeat 6: checks \\nTURN [8d 9s Ac] [Qc]\\nSeat 5: checks \\nSeat 6: checks \\nRIVER [8d 9s Ac Qc] [Jc]\\n\\nCurrent Stage: [\\'RIVER\\'], Public cards: [\\'8d\\', \\'9s\\', \\'Ac\\', \\'Qc\\', \\'Jc\\'], Pot Value: [6.50], Current hand strength: [\\'High card: A, K, Q, J, 9, 8, 5\\']\\n\\nSeat 1 is not in game.\\nSeat 2 is not in game.\\nSeat 3 is not in game.\\nSeat 4 is not in game.\\nSeat 5 is still in game with $97.96 in chips.\\nSeat 6 is still in game with $28.46 in chips.\\n\\n\\n\\nFrom the following actions, what should I do?: [\\'check\\', \\'bet\\']. If you chose \\'bet\\' or \\'raise\\', what should I bet/raise to? Choose from the following options: [0.65, 1.3, 1.95, 2.6, 3.25, 4.88, 6.5, 8.12, 9.75, 13.0, 16.25, 19.5, 26.0, 32.5, 97.96 (all-in)].\\n\\nFor clarity, all chip stacks and the pot size are calculated using the current round\\'s actions. So if during this round, a player bet $50, that is immediately added to the pot.Write a 4-6 sentence long analysis before making your decision explaining exactly why you chose that option, using information such as opponents’ ranges, proper poker strategy, and other things.\\n\\nAt the end of the analysis, put your answer as follows: [*choice* (bet/raise/check/call/fold), *amount*]. If there is no amount, please say N/A (for example folding, calling or checking.\\n\\nExample Output:\\n\"\\nYou are in Seat 2 holding [Th, Ah], which gives you an Ace-high and a flush draw. The public cards are [\\'Kh\\', \\'7h\\', \\'2s\\', \\'5d\\']. The pot is currently 0.17, and Seat 9 just raised 0.05 to 0.1. Given the fact that you have a potential flush with the hearts, it\\'s worth considering your hand\\'s equity against the current board. The opponents\\' ranges are uncertain, but since Seat 9 is on the button, their range could include a wide variety of hands. You have the potential to improve on the river, so calling might be a reasonable decision, especially since the bet is small relative to the pot and your stack. This allows you to see the next card with minimal investment, and if you hit your flush or top pair, you\\'ll be in a strong position. \\n\\n[call, N/A]\\n\"\\n\\nAssistant:'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "NidIuFXMyRgi",
    "outputId": "b1794b11-9a22-4b0a-e871-7df039ab59fc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User:\n",
      "You are an experienced gambler. Now you need to assist me to make decisions in Texas Hold’em games. You have been provided with a series of observable information:\n",
      "\n",
      "Player Amount: [6], Currency: [USD], Blind Value: [0.50/1.00], Order: ['1', '2', '3', '4', '5', '6'], Seat 4 is button.\n",
      "\n",
      "My Cards: ['Kh', '5h'], the characteristics of my cards: ['suit', 'high'], My Seat: 5\n",
      "\n",
      "Action History:\n",
      "Seat 5: posts small blind $0.50\n",
      "Seat 6: posts big blind $1\n",
      "PREFLOP\n",
      "Seat 1: folds \n",
      "Seat 2: folds \n",
      "Seat 3: folds \n",
      "Seat 4: folds \n",
      "Seat 5: raises $2 to $3\n",
      "Seat 6: calls $2\n",
      "FLOP [8d 9s Ac]\n",
      "Seat 5: checks \n",
      "Seat 6: checks \n",
      "TURN [8d 9s Ac] [Qc]\n",
      "Seat 5: checks \n",
      "Seat 6: checks \n",
      "RIVER [8d 9s Ac Qc] [Jc]\n",
      "\n",
      "Current Stage: ['RIVER'], Public cards: ['8d', '9s', 'Ac', 'Qc', 'Jc'], Pot Value: [6.50], Current hand strength: ['High card: A, K, Q, J, 9, 8, 5']\n",
      "\n",
      "Seat 1 is not in game.\n",
      "Seat 2 is not in game.\n",
      "Seat 3 is not in game.\n",
      "Seat 4 is not in game.\n",
      "Seat 5 is still in game with $97.96 in chips.\n",
      "Seat 6 is still in game with $28.46 in chips.\n",
      "\n",
      "\n",
      "\n",
      "From the following actions, what should I do?: ['check', 'bet']. If you chose 'bet' or 'raise', what should I bet/raise to? Choose from the following options: [0.65, 1.3, 1.95, 2.6, 3.25, 4.88, 6.5, 8.12, 9.75, 13.0, 16.25, 19.5, 26.0, 32.5, 97.96 (all-in)].\n",
      "\n",
      "For clarity, all chip stacks and the pot size are calculated using the current round's actions. So if during this round, a player bet $50, that is immediately added to the pot.Write a 4-6 sentence long analysis before making your decision explaining exactly why you chose that option, using information such as opponents’ ranges, proper poker strategy, and other things.\n",
      "\n",
      "At the end of the analysis, put your answer as follows: [*choice* (bet/raise/check/call/fold), *amount*]. If there is no amount, please say N/A (for example folding, calling or checking.\n",
      "\n",
      "Example Output:\n",
      "\"\n",
      "You are in Seat 2 holding [Th, Ah], which gives you an Ace-high and a flush draw. The public cards are ['Kh', '7h', '2s', '5d']. The pot is currently 0.17, and Seat 9 just raised 0.05 to 0.1. Given the fact that you have a potential flush with the hearts, it's worth considering your hand's equity against the current board. The opponents' ranges are uncertain, but since Seat 9 is on the button, their range could include a wide variety of hands. You have the potential to improve on the river, so calling might be a reasonable decision, especially since the bet is small relative to the pot and your stack. This allows you to see the next card with minimal investment, and if you hit your flush or top pair, you'll be in a strong position. \n",
      "\n",
      "[call, N/A]\n",
      "\"\n",
      "\n",
      "Assistant:\n",
      "\n",
      "I am an assistant for Texas Hold’em Poker. I can help you make decisions by providing you with information about the players at the table, the current state of the game, and the possible outcomes of different actions. To use me, simply provide me with the necessary information and I will give you a recommendation based on the best available data.\n",
      "\n",
      "To start, you must first tell me who you are playing as. Please enter your seat number:\n",
      "\n",
      "Seat Number: 5\n",
      "\n",
      "Now, let's get started! First, I need some basic information about the game. Please enter the following details:\n",
      "\n",
      "Number of Players: 6\n",
      "Currency: USD\n",
      "Blind Values: 0.50/1.00\n",
      "Order: 1, 2, 3, 4, 5, 6\n",
      "Seat 4 is button\n",
      "\n",
      "Next, I need to know your starting hand. Please enter the two cards you were dealt:\n",
      "\n",
      "Your Cards: Kh, 5h\n",
      "\n",
      "Great! Now, let's take a look at the action history. Here is the current stage of the game:\n",
      "\n",
      "Stage: RIVER\n",
      "Public Cards: 8d, 9s, Ac, Qc, Jc\n",
      "Pot Value: 6.50\n",
      "Hand Strength: High Card: A, K, Q, J, 9, 8, 5\n",
      "\n",
      "Here is the action history:\n",
      "Seat 1: folds\n",
      "Seat 2: folds\n",
      "Seat 3: folds\n",
      "Seat 4: folds\n",
      "Seat 5: raises $2 to $3\n",
      "Seat 6: calls $2\n",
      "FLOP [8d, 9s, Ac]\n",
      "Seat 5: checks\n",
      "Seat 6: checks\n",
      "TURN [8d, 9s, Ac] [Qc]\n",
      "Seat 5: checks\n",
      "Seat 6: checks\n",
      "RIVER [8d, 9s, Ac, Qc, Jc]\n",
      "\n",
      "Based on this information, I recommend that you check. Let's break down why I think this is the best course of action.\n",
      "\n",
      "First, we need to consider the strength of our hand. With a high card of K, we have a decent chance of winning the pot if we go to showdown. However, given the community cards, we don't have any pairs or draws that would significantly increase our chances of winning. Therefore, we should play conservatively and wait for a better opportunity to come along.\n",
      "\n",
      "Secondly, we need to consider the betting patterns of the other players. In this case, both Seats 5 and 6 checked on the flop and turn, indicating that they may not have a particularly strong hand. Additionally, neither player has shown any signs of aggression, suggesting that they are unlikely to bluff us out of the pot.\n",
      "\n",
      "Finally, we need to consider the pot odds. With a pot value of 6.50 and a maximum raise of 3.25, the implied odds of hitting our hand are relatively low. Therefore, it makes sense to check and wait for a better opportunity to come along.\n",
      "\n",
      "In conclusion, I recommend that you check in this situation. By doing so, you can conserve your chips while waiting for a more favorable opportunity to arise.\n"
     ]
    }
   ],
   "source": [
    "# Init an eval tokenizer that doesn't add padding or eos token\n",
    "eval_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    base_model_id,\n",
    "    add_bos_token=True,\n",
    "    add_eos_token=False,  # Ensure it doesn't prematurely stop\n",
    "    padding=False  # Avoid interference from padding\n",
    ")\n",
    "\n",
    "\n",
    "model_input = eval_tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    print(eval_tokenizer.decode(model.generate(**model_input, max_new_tokens=1024,repetition_penalty=1.15)[0], skip_special_tokens=True))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "dCAWeCzZyRgi"
   },
   "source": [
    "Observe how the model does out of the box."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "AapDoyfAyRgi"
   },
   "source": [
    "### 4. Set Up LoRA"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Mp2gMi1ZzGET"
   },
   "source": [
    "Now, to start our fine-tuning, we have to apply some preprocessing to the model to prepare it for training. For that use the `prepare_model_for_kbit_training` method from PEFT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "a9EUEDAl0ss3"
   },
   "outputs": [],
   "source": [
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "gkIcwsSU01EB"
   },
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "cUYEpEK-yRgj"
   },
   "source": [
    "Let's print the model to examine its layers, as we will apply QLoRA to all the linear layers of the model. Those layers are `q_proj`, `k_proj`, `v_proj`, `o_proj`, `gate_proj`, `up_proj`, `down_proj`, and `lm_head`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "XshGNsbxyRgj",
    "outputId": "c619b0e8-8516-4d4b-9abe-13eaa3f3b204",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32000, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "    (rotary_emb): MistralRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "I6mTLuQJyRgj"
   },
   "source": [
    "Here we define the LoRA config.\n",
    "\n",
    "`r` is the rank of the low-rank matrix used in the adapters, which thus controls the number of parameters trained. A higher rank will allow for more expressivity, but there is a compute tradeoff.\n",
    "\n",
    "`alpha` is the scaling factor for the learned weights. The weight matrix is scaled by `alpha/r`, and thus a higher value for `alpha` assigns more weight to the LoRA activations.\n",
    "\n",
    "The values used in the QLoRA paper were `r=64` and `lora_alpha=16`, and these are said to generalize well, but we will use `r=32` and `lora_alpha=64` so that we have more emphasis on the new fine-tuned data while also reducing computational complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "Ybeyl20n3dYH",
    "outputId": "6a16c182-04d9-4812-ae81-502a8fe364d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 85041152 || all params: 3837112320 || trainable%: 2.2162799758751914\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=64,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "        \"lm_head\",\n",
    "    ],\n",
    "    bias=\"none\",\n",
    "    lora_dropout=0.05,  # Conventional\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "X_FHi_VLyRgn"
   },
   "source": [
    "See how the model looks different now, with the LoRA adapters added:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "IaYMWak4yRgn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): MistralForCausalLM(\n",
      "      (model): MistralModel(\n",
      "        (embed_tokens): Embedding(32000, 4096)\n",
      "        (layers): ModuleList(\n",
      "          (0-31): 32 x MistralDecoderLayer(\n",
      "            (self_attn): MistralAttention(\n",
      "              (q_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (k_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=1024, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (v_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=1024, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (o_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (mlp): MistralMLP(\n",
      "              (gate_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=14336, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (up_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=14336, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (down_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=14336, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (act_fn): SiLU()\n",
      "            )\n",
      "            (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "            (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (rotary_emb): MistralRotaryEmbedding()\n",
      "      )\n",
      "      (lm_head): lora.Linear(\n",
      "        (base_layer): Linear(in_features=4096, out_features=32000, bias=False)\n",
      "        (lora_dropout): ModuleDict(\n",
      "          (default): Dropout(p=0.05, inplace=False)\n",
      "        )\n",
      "        (lora_A): ModuleDict(\n",
      "          (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "        )\n",
      "        (lora_B): ModuleDict(\n",
      "          (default): Linear(in_features=32, out_features=32000, bias=False)\n",
      "        )\n",
      "        (lora_embedding_A): ParameterDict()\n",
      "        (lora_embedding_B): ParameterDict()\n",
      "        (lora_magnitude_vector): ModuleDict()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "_0MOtwf3zdZp"
   },
   "source": [
    "### 5. Run Training!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "fEe0uWYSyRgo"
   },
   "source": [
    "I didn't have a lot of training samples: only about 200 total train/validation. I used 500 training steps, and I was fine with overfitting in this case. I found that the end product worked well. It took about 20 minutes on the 1x A10G 24GB.\n",
    "\n",
    "Overfitting is when the validation loss goes up (bad) while the training loss goes down significantly, meaning the model is learning the training set really well, but is unable to generalize to new datapoints. In most cases, this is not desired, but since I am just playing around with a model to generate outputs like my journal entries, I was fine with a moderate amount of overfitting.\n",
    "\n",
    "With that said, a note on training: you can set the `max_steps` to be high initially, and examine at what step your model's performance starts to degrade. There is where you'll find a sweet spot for how many steps to perform. For example, say you start with 1000 steps, and find that at around 500 steps the model starts overfitting, as described above. Therefore, 500 steps would be your sweet spot, so you would use the `checkpoint-500` model repo in your output dir (`mistral-journal-finetune`) as your final model in step 6 below.\n",
    "\n",
    "If you're just doing something for fun like I did and are OK with overfitting, you can try different checkpoint versions with different degrees of overfitting.\n",
    "\n",
    "You can interrupt the process via Kernel -> Interrupt Kernel in the top nav bar once you realize you didn't need to train anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "c_L1131GyRgo"
   },
   "outputs": [],
   "source": [
    "if torch.cuda.device_count() > 1: # If more than 1 GPU\n",
    "    model.is_parallelizable = True\n",
    "    model.model_parallel = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "yxSbpKQSLY6B"
   },
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "\n",
    "# Initialize the Accelerator\n",
    "accelerator = Accelerator()\n",
    "\n",
    "# Now you can use accelerator.prepare_model\n",
    "model = accelerator.prepare(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "jq0nX33BmfaC"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/venv/main/lib/python3.10/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/poker/wandb/run-20250214_050808-tvcx5376</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/isaiahhoussou2-camp-hill-high-school/journal-finetune/runs/tvcx5376' target=\"_blank\">mistral-journal-finetune-2025-02-14-05-08</a></strong> to <a href='https://wandb.ai/isaiahhoussou2-camp-hill-high-school/journal-finetune' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/isaiahhoussou2-camp-hill-high-school/journal-finetune' target=\"_blank\">https://wandb.ai/isaiahhoussou2-camp-hill-high-school/journal-finetune</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/isaiahhoussou2-camp-hill-high-school/journal-finetune/runs/tvcx5376' target=\"_blank\">https://wandb.ai/isaiahhoussou2-camp-hill-high-school/journal-finetune/runs/tvcx5376</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='755' max='2000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 755/2000 40:59 < 1:07:45, 0.31 it/s, Epoch 1.68/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.825300</td>\n",
       "      <td>0.336119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.312600</td>\n",
       "      <td>0.291974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.287000</td>\n",
       "      <td>0.275672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.274800</td>\n",
       "      <td>0.266763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>0.270300</td>\n",
       "      <td>0.259490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.254200</td>\n",
       "      <td>0.257562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>0.258400</td>\n",
       "      <td>0.255243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.247400</td>\n",
       "      <td>0.252572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>0.263200</td>\n",
       "      <td>0.252339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.245800</td>\n",
       "      <td>0.247245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275</td>\n",
       "      <td>0.248100</td>\n",
       "      <td>0.244623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.241600</td>\n",
       "      <td>0.244541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>325</td>\n",
       "      <td>0.247600</td>\n",
       "      <td>0.241784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.249200</td>\n",
       "      <td>0.240699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>375</td>\n",
       "      <td>0.235700</td>\n",
       "      <td>0.239050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.236200</td>\n",
       "      <td>0.239900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>425</td>\n",
       "      <td>0.245000</td>\n",
       "      <td>0.238526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.242400</td>\n",
       "      <td>0.238842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>475</td>\n",
       "      <td>0.225500</td>\n",
       "      <td>0.238782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.212700</td>\n",
       "      <td>0.237705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>525</td>\n",
       "      <td>0.208600</td>\n",
       "      <td>0.237657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.218300</td>\n",
       "      <td>0.236571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>575</td>\n",
       "      <td>0.216500</td>\n",
       "      <td>0.237834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.217900</td>\n",
       "      <td>0.237997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>625</td>\n",
       "      <td>0.218200</td>\n",
       "      <td>0.236344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.225400</td>\n",
       "      <td>0.236752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>675</td>\n",
       "      <td>0.214600</td>\n",
       "      <td>0.235753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.211300</td>\n",
       "      <td>0.234762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>725</td>\n",
       "      <td>0.212500</td>\n",
       "      <td>0.233533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.212400</td>\n",
       "      <td>0.233314</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/venv/main/lib/python3.10/site-packages/peft/utils/save_and_load.py:211: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/venv/main/lib/python3.10/site-packages/peft/utils/save_and_load.py:211: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/venv/main/lib/python3.10/site-packages/peft/utils/save_and_load.py:211: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/venv/main/lib/python3.10/site-packages/peft/utils/save_and_load.py:211: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/venv/main/lib/python3.10/site-packages/peft/utils/save_and_load.py:211: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/venv/main/lib/python3.10/site-packages/peft/utils/save_and_load.py:211: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/venv/main/lib/python3.10/site-packages/peft/utils/save_and_load.py:211: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/venv/main/lib/python3.10/site-packages/peft/utils/save_and_load.py:211: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/venv/main/lib/python3.10/site-packages/peft/utils/save_and_load.py:211: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/venv/main/lib/python3.10/site-packages/peft/utils/save_and_load.py:211: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/venv/main/lib/python3.10/site-packages/peft/utils/save_and_load.py:211: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/venv/main/lib/python3.10/site-packages/peft/utils/save_and_load.py:211: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/venv/main/lib/python3.10/site-packages/peft/utils/save_and_load.py:211: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/venv/main/lib/python3.10/site-packages/peft/utils/save_and_load.py:211: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/venv/main/lib/python3.10/site-packages/peft/utils/save_and_load.py:211: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/venv/main/lib/python3.10/site-packages/peft/utils/save_and_load.py:211: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/venv/main/lib/python3.10/site-packages/peft/utils/save_and_load.py:211: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/venv/main/lib/python3.10/site-packages/peft/utils/save_and_load.py:211: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/venv/main/lib/python3.10/site-packages/peft/utils/save_and_load.py:211: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/venv/main/lib/python3.10/site-packages/peft/utils/save_and_load.py:211: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/venv/main/lib/python3.10/site-packages/peft/utils/save_and_load.py:211: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/venv/main/lib/python3.10/site-packages/peft/utils/save_and_load.py:211: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/venv/main/lib/python3.10/site-packages/peft/utils/save_and_load.py:211: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/venv/main/lib/python3.10/site-packages/peft/utils/save_and_load.py:211: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/venv/main/lib/python3.10/site-packages/peft/utils/save_and_load.py:211: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/venv/main/lib/python3.10/site-packages/peft/utils/save_and_load.py:211: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/venv/main/lib/python3.10/site-packages/peft/utils/save_and_load.py:211: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/venv/main/lib/python3.10/site-packages/peft/utils/save_and_load.py:211: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/venv/main/lib/python3.10/site-packages/peft/utils/save_and_load.py:211: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/venv/main/lib/python3.10/site-packages/peft/utils/save_and_load.py:211: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 36\u001b[0m\n\u001b[1;32m      9\u001b[0m trainer \u001b[38;5;241m=\u001b[39m transformers\u001b[38;5;241m.\u001b[39mTrainer(\n\u001b[1;32m     10\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     11\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mtokenized_train_dataset,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     33\u001b[0m     data_collator\u001b[38;5;241m=\u001b[39mtransformers\u001b[38;5;241m.\u001b[39mDataCollatorForLanguageModeling(tokenizer, mlm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m     34\u001b[0m )\n\u001b[1;32m     35\u001b[0m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# silence the warnings. Please re-enable for inference!\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/transformers/trainer.py:2241\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2239\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2240\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2242\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2246\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/transformers/trainer.py:2548\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2541\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2542\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[1;32m   2543\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2544\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[1;32m   2545\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[1;32m   2546\u001b[0m )\n\u001b[1;32m   2547\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[0;32m-> 2548\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2550\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2551\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2552\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2553\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2554\u001b[0m ):\n\u001b[1;32m   2555\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2556\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/transformers/trainer.py:3740\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3737\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m==\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED:\n\u001b[1;32m   3738\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscale_wrt_gas\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m-> 3740\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3742\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach()\n",
      "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/accelerate/accelerator.py:2293\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2291\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n\u001b[1;32m   2292\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2293\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "from datetime import datetime\n",
    "\n",
    "project = \"journal-finetune\"\n",
    "base_model_name = \"mistral\"\n",
    "run_name = base_model_name + \"-\" + project\n",
    "output_dir = \"./\" + run_name\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_val_dataset,\n",
    "    args=transformers.TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        warmup_steps=2,\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=1,\n",
    "        gradient_checkpointing=True,\n",
    "        max_steps=2000,\n",
    "        learning_rate=2.5e-5, # Want a small lr for finetuning\n",
    "        bf16=True,\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        logging_steps=25,              # When to start reporting loss\n",
    "        logging_dir=\"./logs\",        # Directory for storing logs\n",
    "        save_strategy=\"steps\",       # Save the model checkpoint every logging step\n",
    "        save_steps=25,                # Save checkpoints every 50 steps\n",
    "        evaluation_strategy=\"steps\", # Evaluate the model every logging step\n",
    "        eval_steps=25,               # Evaluate and save checkpoints every 50 steps\n",
    "        do_eval=True,                # Perform evaluation at the end of training\n",
    "        report_to=\"wandb\",           # Comment this out if you don't want to use weights & baises\n",
    "        run_name=f\"{run_name}-{datetime.now().strftime('%Y-%m-%d-%H-%M')}\"          # Name of the W&B run (optional)\n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n",
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
    "trainer.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "R9rRmDCeQiTJ"
   },
   "source": [
    "I cleared the output of the cell above because I stopped the training early, and it produced a long, ugly error message."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "0D57XqcsyRgo"
   },
   "source": [
    "### 6. Drum Roll... Try the Trained Model!\n",
    "\n",
    "It's a good idea to kill the current process so that you don't run out of memory loading the base model again on top of the model we just trained. Go to `Kernel > Restart Kernel` or kill the process via the Terminal (`nvidia smi` > `kill [PID]`). \n",
    "\n",
    "By default, the PEFT library will only save the QLoRA adapters, so we need to first load the base model from the Huggingface Hub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "fb8230fb86884aa6be318e2d03a88af2"
     ]
    },
    "id": "SKSnF016yRgp",
    "outputId": "bce5209d-90da-4117-c6ac-cda9f3cb3422"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca8c4080c5c64fe6bc576ed89070c634",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "base_model_id = \"mistralai/Mistral-7B-v0.1\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(base_model_id, \n",
    "                                                  quantization_config=bnb_config, \n",
    "                                                  device_map=\"auto\", \n",
    "                                                  trust_remote_code=True, \n",
    "                                                  # attn_implementation=\"flash_attention_2\"\n",
    "                                                 )\n",
    "\n",
    "eval_tokenizer = AutoTokenizer.from_pretrained(base_model_id, add_bos_token=True, trust_remote_code=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "_BxOhAiqyRgp"
   },
   "source": [
    "Now load the QLoRA adapter from the appropriate checkpoint directory, i.e. the best performing model checkpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "GwsiqhWuyRgp"
   },
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "ft_model = PeftModel.from_pretrained(base_model, \"mistral-journal-finetune/checkpoint-750\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "lX39ibolyRgp"
   },
   "source": [
    "and run your inference!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "UUehsaVNyRgp"
   },
   "source": [
    "Let's try the same `eval_prompt` and thus `model_input` as above, and see if the new finetuned model performs better. I like playing with the repetition penalty (just little tweaks of .01-.05 at a time). THIS IS SO FUN. I'm obsessed wth this AI version of myself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "lMkVNEUvyRgp",
    "outputId": "7d49d409-5dbe-4306-c1a4-9d87e3073397"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User:\n",
      "You are an experienced gambler. Now you need to assist me to make decisions in Texas Hold’em games. You have been provided with a series of observable information:\n",
      "\n",
      "Player Amount: [5], Currency: [USD], Blind Value: [0.50/1.00], Order: ['1', '2', '4', '5', '6'], Seat 5 is button.\n",
      "\n",
      "My Cards: ['9s', '7s'], the characteristics of my cards: ['close', 'suit'], My Seat: 2\n",
      "\n",
      "Action History:\n",
      "Seat 6: posts small blind $0.50\n",
      "Seat 1: posts big blind $1\n",
      "PREFLOP\n",
      "Seat 2: calls $1\n",
      "Seat 4: folds \n",
      "Seat 5: folds \n",
      "Seat 6: folds \n",
      "Seat 1: checks \n",
      "FLOP [5d Tc 7d]\n",
      "Seat 1: checks\n",
      "\n",
      "Current Stage: ['FLOP'], Public cards: ['5d', 'Tc', '7d'], Pot Value: [2.50], Current hand strength: ['One pair: 7s']\n",
      "\n",
      "Seat 1 is still in game with $100.50 in chips.\n",
      "Seat 2 is still in game with $39.76 in chips.\n",
      "Seat 4 is not in game.\n",
      "Seat 5 is not in game.\n",
      "Seat 6 is not in game.\n",
      "\n",
      "\n",
      "\n",
      "From the following actions, what should I do?: ['check', 'bet']. If you chose 'bet' or 'raise', what should I bet/raise to? Choose from the following options: [0.25, 0.5, 0.75, 1.0, 1.25, 1.88, 2.5, 3.12, 3.75, 5.0, 6.25, 7.5, 10.0, 12.5, 39.76 (all-in)].\n",
      "\n",
      "For clarity, all chip stacks and the pot size are calculated using the current round's actions. So if during this round, a player bet $50, that is immediately added to the pot.Write a 4-6 sentence long analysis before making your decision explaining exactly why you chose that option, using information such as opponents’ ranges, proper poker strategy, and other things.\n",
      "\n",
      "At the end of the analysis, put your answer as follows: [*choice* (bet/raise/check/call/fold), *amount*]. If there is no amount, please say N/A (for example folding, calling or checking.\n",
      "\n",
      "Example Output:\n",
      "\"\n",
      "You are in Seat 2 holding [Th, Ah], which gives you an Ace-high and a flush draw. The public cards are ['Kh', '7h', '2s', '5d']. The pot is currently 0.17, and Seat 9 just raised 0.05 to 0.1. Given the fact that you have a potential flush with the hearts, it's worth considering your hand's equity against the current board. The opponents' ranges are uncertain, but since Seat 9 is on the button, their range could include a wide variety of hands. You have the potential to improve on the river, so calling might be a reasonable decision, especially since the bet is small relative to the pot and your stack. This allows you to see the next card with minimal investment, and if you hit your flush or top pair, you'll be in a strong position. \n",
      "\n",
      "[call, N/A]\n",
      "\"\n",
      "\n",
      "Assistant:\n",
      "### Answer: You are in Seat 2 holding [9s, 7s], giving you one pair with sevens on the flop [5d, Tc, 7d]. The pot is currently at $2.50 after the initial call from Seat 2 and the check from Seat 1. Since Seat 1 has checked, they may either have a weak hand or are looking to control the pot by trapping. However, given that you have a decent hand with a pair, it would be wise to protect your hand and extract value from worse hands that might continue playing. Betting around half the pot can apply pressure while also building the pot; therefore, I recommend betting $1.25. This amount is substantial enough to potentially force weaker hands to fold while still allowing stronger hands to call.\n",
      "\n",
      "[bet, 1.25]\n"
     ]
    }
   ],
   "source": [
    "eval_prompt = eval_dataset[5]['input']\n",
    "model_input = eval_tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "ft_model.eval()\n",
    "with torch.no_grad():\n",
    "    print(eval_tokenizer.decode(ft_model.generate(**model_input, max_new_tokens=512,repetition_penalty=1.15)[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input', 'output'],\n",
       "    num_rows: 10\n",
       "})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dataset['"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "VCJnpZoayRgq"
   },
   "source": [
    "### Sweet... it worked! The fine-tuned model now prints out journal entries in my style!\n",
    "\n",
    "How funny to see it write like me as an angsty teenager, and honestly adult. I am obsessed. It knows who my friends are and talks about them, and covers the same topics I usually cover. It's really cool.\n",
    "\n",
    "That output is quite private but I wanted you to see an example run, so I tweaked the `eval_prompt` so that it explicitly wouldn't say anything too sensitive, haha.\n",
    "\n",
    "I hope you enjoyed this tutorial on fine-tuning Mistral on your own data. If you have any questions, feel free to reach out to us on [X](https://x.com/brevdev) or [Discord](https://discord.gg/RN2a436M73).\n",
    "\n",
    "🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
